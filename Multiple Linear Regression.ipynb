{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "this OLS multiple linear regression implementation is fitted on Fisher's Iris data set.\n",
    "We will try to classify flowers into 3 types based on their sepal length, sepal width,\n",
    "petal length, petal length in cm. One class is linearly separable from the other 2;\n",
    "the latter are NOT linearly separable from each other.\n",
    "Since the focus of this code is the algorithm itself rather than the data,\n",
    "we will spend no time analysing, or visualizing the data.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "loading the data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "X = data['data']\n",
    "predictor_num = len(data['feature_names'])\n",
    "example_num = X.shape[0]\n",
    "\n",
    "Y = data['target']\n",
    "Y = Y[:, None]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our X variable is a matrix consisting of $m$ examples and $n$ features. Each of the $m$ examples\n",
    "corresponds to an out an entry in the response vector.\n",
    "\n",
    "$X = \\begin{bmatrix} x_{1,1} & x_{1,2} & \\cdots & x_{1,n}\\\\\n",
    " x_{2,1} & x_{2,2} & \\cdots & x_{2,n} \\\\\n",
    "  x_{3,1} & x_{3,2} & \\cdots & x_{3,n} \\\\\n",
    "   \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\     x_{m,1} & x_{m,2} & \\cdots & x_{m,n}  \\end{bmatrix},\n",
    " Y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3\\\\ \\vdots \\\\ y_n\\end{bmatrix} $\n",
    "\n",
    "In order to predict the response variable, we want to fit a line through our\n",
    "data.\n",
    "\n",
    "$Y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_nx_n$\n",
    "\n",
    "In order for us to make the equation more compact we will represent it as the dot product of two matrices. First,\n",
    "we will put all the coefficients in a vector. Then, we will put another vector consisting of\n",
    "1s in the $X$ matrix; this will allow us to include the y-intercept term in the dot product. Thus, $X$ will become:\n",
    "\n",
    "$X = \\begin{bmatrix} 1& x_{1,1} & x_{1,2} & \\cdots & x_{1,n}\\\\\n",
    " 1 & x_{2,1} & x_{2,2} & \\cdots & x_{2,n} \\\\\n",
    "  1 & x_{3,1} & x_{3,2} & \\cdots & x_{3,n} \\\\\n",
    "   \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_{m,1} & x_{m,2} & \\cdots & x_{m,n}  \\end{bmatrix}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# adding another feature vector of value 1\n",
    "X = np.hstack(((np.ones((example_num, 1))), X))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we can rewrite the previous equation as follows:\n",
    "\n",
    "$Y = \\beta X$\n",
    "\n",
    "Neat! Now before we look for a way to get the values of the coefficients, we have to answer an important question.\n",
    "How will we measure the error of our regression line? We have to find a way to measure the error in order for us to find\n",
    "the coefficients that minimize it.\n",
    "Since this is an ordinary least squares(OLS) algorithm,\n",
    "we will take the average of the squared residuals.\n",
    "\n",
    "$Cost(\\beta) = \\frac{1}{m} \\sum_{i=1}^{m}(\\hat y - y)^2$\n",
    "\n",
    "Where $\\hat y$ is the predicted response for a specific example.\n",
    "\n",
    "Again, since we are working with vectors, there is a nicer way to write the error equation.\n",
    "\n",
    "$Cost(\\beta) = \\frac{1}{m} (\\hat Y - Y)^T(\\hat Y - Y)$\n",
    "\n",
    "We want the values of the coefficients inside $\\beta$ that minimize this error function.\n",
    " Since our error equation is quadratic, we know that it has a global minima of $0$.\n",
    " That also means that the derivative of the cost function at that global minima is $0$.\n",
    "  We can use differential calculus to find the value of $\\beta$ that satisfies that.\n",
    "\n",
    "  $\\frac{\\partial}{\\partial \\beta} Cost(\\beta) = \\frac{\\partial}{\\partial \\beta}\\frac{1}{m}[ (X\\beta - Y)^T(X\\beta - Y)]$\n",
    "\n",
    "  Note that, $\\hat Y = X\\beta$.\n",
    "\n",
    "  And since , $(A + B)^T = A^T + B^T$.we can simplify even further:\n",
    "\n",
    "  $\\begin{align}\\frac{\\partial}{\\partial \\beta} Cost(\\beta)  &= \\frac{\\partial}{\\partial \\beta}\\frac{1}{m} [((X\\beta)^T - Y^T)(X\\beta - Y)] \\\\\n",
    "  &= \\frac{\\partial}{\\partial \\beta}\\frac{1}{m} [(X\\beta)^T(X\\beta) - (X\\beta)^TY - Y^T(X\\beta) - Y^TY]\n",
    "  \\end{align}$\n",
    "\n",
    "  W know that $X\\beta$ and $Y$ are both Vectors. So, the property\n",
    "  $A^TB = B^TA$ holds.\n",
    "\n",
    "  $\\begin{align}\\frac{\\partial}{\\partial \\beta} Cost(\\beta)  &= \\frac{\\partial}{\\partial \\beta}\\frac{1}{m}[ (X\\beta)^T(X\\beta) - 2(X\\beta)^TY - Y^TY] \\\\\n",
    "  &= \\frac{1}{m}\\frac{\\partial}{\\partial \\beta}[(X\\beta)^T(X\\beta) - 2(X\\beta)^TY] - \\frac{\\partial}{\\partial \\beta}Y^TY \\\\\n",
    "  &= \\frac{1}{m}\\frac{\\partial}{\\partial \\beta}[(X\\beta)^T(X\\beta) - 2(X\\beta)^TY] - 0 \\\\\n",
    "  &= \\frac{1}{m}[\\frac{\\partial}{\\partial \\beta}[\\beta^TX^TX\\beta] -  \\frac{\\partial}{\\partial \\beta} [2\\beta^TX^TY]]\n",
    "  \\end{align}$\n",
    "\n",
    "  since we are trying to minimize the cost, we can set the LHS $\\frac{\\partial}{\\partial \\beta} Cost(\\beta)$ to $0$.\n",
    "\n",
    "  $\\begin{align}&\\frac{1}{m}[[\\frac{\\partial}{\\partial \\beta}\\beta^TX^TX\\beta] -  \\frac{\\partial}{\\partial \\beta} [2\\beta^TX^TY]] = 0 \\\\\n",
    "  &\\frac{\\partial}{\\partial \\beta}[\\beta^TX^TX\\beta] -  \\frac{\\partial}{\\partial \\beta} [2\\beta^TX^TY] = 0 \\\\\n",
    "  &\\frac{\\partial}{\\partial \\beta}[\\beta^TX^TX\\beta] =  \\frac{\\partial}{\\partial \\beta} [2\\beta^TX^TY]\n",
    "  \\end{align}$\n",
    "\n",
    "  Taking the derivative of the LHS:\n",
    "\n",
    "  $\\begin{align}\n",
    "  &\\frac{\\partial}{\\partial \\beta}[\\beta^TX^TX\\beta]\\\\\n",
    "  &= X^TX\\frac{\\partial}{\\partial \\beta}(\\beta^T\\beta) \\\\\n",
    "  &= 2X^TX\\beta\n",
    "  \\end{align}$\n",
    "\n",
    "  Taking the derivative of the RHS:\n",
    "\n",
    "  $\\begin{align}\n",
    "  &\\frac{\\partial}{\\partial \\beta} [2\\beta^TX^TY]\\\\\n",
    "  &= 2X^TY\\frac{\\partial}{\\partial \\beta} (\\beta^T) \\\\\n",
    "  &= 2X^TY\n",
    "  \\end{align}$\n",
    "\n",
    "  Now that we computed our derivatives we can finally simplify the full equation to get $\\beta$\n",
    "\n",
    "  $\\begin{align}\n",
    "  &2X^TX\\beta = 2X^TY \\\\\n",
    "  &\\beta = X^TY \\\\\n",
    "  &\\beta = (X^TX)^{-1}X^TY\n",
    "  \\end{align}$\n",
    "  \n",
    "  We finally derived the normal equation. We will use it to find the value of our $\\beta$ vector\n",
    "  that minimizes the error!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.18649525]\n",
      " [-0.11190585]\n",
      " [-0.04007949]\n",
      " [ 0.22864503]\n",
      " [ 0.60925205]]\n"
     ]
    }
   ],
   "source": [
    "# calculating the slopes according to the normal equation\n",
    "XTXInv = np.linalg.inv(np.dot(X.T,X))\n",
    "XTY = np.dot(X.T, Y)\n",
    "Beta = np.dot(XTXInv,XTY)\n",
    "\n",
    "print(Beta)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " And now that we got our slopes, we can predict\n",
    " the response $Y$ with our linear regression equation:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$Y = X\\beta$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "Prediction = np.dot(X,Beta)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's assess the quality of our fit!\n",
    "\n",
    "First, we will compute the $R^2$ statistic to find out the proportion of variability in $Y$\n",
    "explained by our model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9303939218549564\n"
     ]
    }
   ],
   "source": [
    "# the amount of variability inherent in\n",
    "# the response before the regression is performed\n",
    "TSS = float(np.dot((Y - np.mean(Y)).T,(Y - np.mean(Y))))\n",
    "\n",
    "# the amount of unexplained variability\n",
    "# left after the regression\n",
    "RSS = float(np.dot((Prediction - Y).T,(Prediction - Y)))\n",
    "\n",
    "# proportion of variability explained by our model\n",
    "R2 = 1 - RSS/TSS\n",
    "\n",
    "print(R2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Not bad! Our model explains 93% of the variability in $Y$\n",
    ".\n",
    "\n",
    "Now we will use the Residual standard error (RSE), adjusted to be unbiased, to find the\n",
    "standard deviation of our error."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2190985892792741\n"
     ]
    }
   ],
   "source": [
    "RSE = np.sqrt(RSS / (example_num - predictor_num - 1))\n",
    "\n",
    "print(RSE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A standard deviation of error $\\approx$ 0.22 is not bad considering\n",
    "that $Y$ has a range $[0, 2]$\n",
    "\n",
    "We could have used gradient descent to find our coefficients,\n",
    "but it would have been computationally expensive compared to using the normal equation for such a small number of predictors."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
